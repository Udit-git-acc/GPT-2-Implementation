{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTLO Programming Assignment\n",
    "## Implementation and Analysis of GPT-2\n",
    "---\n",
    "    Udit Agarwal B20ME076\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q/A Question Section\n",
    "\n",
    "Answer is uploaded in the single doc file at the link: https://docs.google.com/document/d/1C1wd81OzGKpu0z32LKw3_T4Arjf9sIbvYjzyNZjysLE/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 | GPT-2 Model & Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "import requests\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import tiktoken\n",
    "# from bpemb import BPEmb\n",
    "from contextlib import nullcontext\n",
    "\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from transformers import load_tf_weights_in_gpt2\n",
    "from transformers.utils import CONFIG_NAME, WEIGHTS_NAME\n",
    "from transformers import GPT2Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the model architecture and key components like layer norm, multi-head self-attention mechanism, feed-forward networks and positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal Self Attention\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Layer Perceptron  \n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single tansformer decoder block    \n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing/Configuring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass    \n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n",
    "# Defining the GPT model\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def from_pretrained(model_type, override_args=None):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        override_args = override_args or {} # default to empty dict\n",
    "        # only dropout can be overridden see more notes below\n",
    "        assert all(k == 'dropout' for k in override_args)\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        config_args['bias'] = True # always True for GPT model checkpoints\n",
    "        # we can override the dropout rate, if desired\n",
    "        if 'dropout' in override_args:\n",
    "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args['dropout'] = override_args['dropout']\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2ModelInitializer:\n",
    "    def __init__(self, model_args, init_from, out_dir, device, dropout):\n",
    "        self.init_from = init_from\n",
    "        self.model_args = model_args\n",
    "        self.meta_vocab_size = meta_vocab_size\n",
    "        self.out_dir = out_dir\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "        self.checkpoint = None\n",
    "\n",
    "    def initialize_model(self):\n",
    "        # Defining the scratch model\n",
    "        if self.init_from == 'scratch':\n",
    "            print(\"Initializing a new model from scratch\")\n",
    "            if self.meta_vocab_size is None:\n",
    "                print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "            self.model_args['vocab_size'] = self.meta_vocab_size if self.meta_vocab_size is not None else 50304\n",
    "            gptconf = GPTConfig(**self.model_args)\n",
    "            model = GPT(gptconf)\n",
    "\n",
    "        # Loading the .ckpt model and or the models weights from the tensorflow weights\n",
    "        elif self.init_from == 'resume':\n",
    "            print(f\"Resuming training from {self.out_dir}\")\n",
    "            ckpt_path = os.path.join(self.out_dir, 'ckpt.pt')\n",
    "            checkpoint = torch.load(ckpt_path, map_location=self.device)\n",
    "            checkpoint_model_args = checkpoint['model_args']\n",
    "            for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "                self.model_args[k] = checkpoint_model_args[k]\n",
    "            gptconf = GPTConfig(**self.model_args)\n",
    "            model = GPT(gptconf)\n",
    "            state_dict = checkpoint['model']\n",
    "            unwanted_prefix = '_orig_mod.'\n",
    "            for k,v in list(state_dict.items()):\n",
    "                if k.startswith(unwanted_prefix):\n",
    "                    state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "            model.load_state_dict(state_dict)\n",
    "            iter_num = checkpoint['iter_num']\n",
    "            best_val_loss = checkpoint['best_val_loss']\n",
    "            self.checkpoint = checkpoint\n",
    "\n",
    "        # Importing the model weights from hugging face and implementing\n",
    "        elif self.init_from.startswith('gpt2'):\n",
    "            print(f\"Initializing from OpenAI GPT-2 weights: {self.init_from}\")\n",
    "            override_args = dict(dropout=self.dropout)\n",
    "            model = GPT.from_pretrained(self.init_from, override_args)\n",
    "            for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "                self.model_args[k] = getattr(model.config, k)\n",
    "\n",
    "        return model, self.model_args, self.checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4194304 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\d_drive\\Contlo\\ContloFinal.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model_args \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(n_layer\u001b[39m=\u001b[39mn_layer, n_head\u001b[39m=\u001b[39mn_head, n_embd\u001b[39m=\u001b[39mn_embd, block_size\u001b[39m=\u001b[39mblock_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                   bias\u001b[39m=\u001b[39mbias, vocab_size\u001b[39m=\u001b[39m\u001b[39m50257\u001b[39m, dropout\u001b[39m=\u001b[39mdropout) \u001b[39m# start with model_args from command line\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m initializer \u001b[39m=\u001b[39m GPT2ModelInitializer(init_from\u001b[39m=\u001b[39minit_from, model_args\u001b[39m=\u001b[39mmodel_args, out_dir\u001b[39m=\u001b[39mout_dir, device \u001b[39m=\u001b[39m device, dropout \u001b[39m=\u001b[39m dropout)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m model, model_args,_ \u001b[39m=\u001b[39m initializer\u001b[39m.\u001b[39;49minitialize_model()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# crop down the model block size if desired, using model surgery\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m block_size \u001b[39m<\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mblock_size:\n",
      "\u001b[1;32mc:\\d_drive\\Contlo\\ContloFinal.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_args[\u001b[39m'\u001b[39m\u001b[39mvocab_size\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta_vocab_size \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta_vocab_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m50304\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     gptconf \u001b[39m=\u001b[39m GPTConfig(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_args)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     model \u001b[39m=\u001b[39m GPT(gptconf)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Loading the .ckpt model and or the models weights from the tensorflow weights\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_from \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mresume\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;32mc:\\d_drive\\Contlo\\ContloFinal.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39massert\u001b[39;00m config\u001b[39m.\u001b[39mblock_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleDict(\u001b[39mdict\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     wte \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mvocab_size, config\u001b[39m.\u001b[39mn_embd),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     wpe \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mblock_size, config\u001b[39m.\u001b[39mn_embd),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     drop \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39mdropout),\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     h \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([Block(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mn_layer)]),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     ln_f \u001b[39m=\u001b[39m LayerNorm(config\u001b[39m.\u001b[39mn_embd, bias\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mbias),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m ))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mn_embd, config\u001b[39m.\u001b[39mvocab_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# with weight tying when using torch.compile() some warnings get generated:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# \"UserWarning: functional_call was passed multiple values for tied weights.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# This behavior is deprecated and will be an error in future versions\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# not 100% sure what this is, so far seems to be harmless. TODO investigate\u001b[39;00m\n",
      "\u001b[1;32mc:\\d_drive\\Contlo\\ContloFinal.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39massert\u001b[39;00m config\u001b[39m.\u001b[39mblock_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleDict(\u001b[39mdict\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     wte \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mvocab_size, config\u001b[39m.\u001b[39mn_embd),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     wpe \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mblock_size, config\u001b[39m.\u001b[39mn_embd),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     drop \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39mdropout),\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     h \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([Block(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mn_layer)]),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     ln_f \u001b[39m=\u001b[39m LayerNorm(config\u001b[39m.\u001b[39mn_embd, bias\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mbias),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m ))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mn_embd, config\u001b[39m.\u001b[39mvocab_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# with weight tying when using torch.compile() some warnings get generated:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# \"UserWarning: functional_call was passed multiple values for tied weights.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# This behavior is deprecated and will be an error in future versions\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# not 100% sure what this is, so far seems to be harmless. TODO investigate\u001b[39;00m\n",
      "\u001b[1;32mc:\\d_drive\\Contlo\\ContloFinal.ipynb Cell 13\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1 \u001b[39m=\u001b[39m LayerNorm(config\u001b[39m.\u001b[39mn_embd, bias\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mbias)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn \u001b[39m=\u001b[39m CausalSelfAttention(config)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2 \u001b[39m=\u001b[39m LayerNorm(config\u001b[39m.\u001b[39mn_embd, bias\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mbias)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp \u001b[39m=\u001b[39m MLP(config)\n",
      "\u001b[1;32mc:\\d_drive\\Contlo\\ContloFinal.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# causal mask to ensure that attention is only applied to the left in the input sequence\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\u001b[39m\"\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mtril(torch\u001b[39m.\u001b[39;49mones(config\u001b[39m.\u001b[39;49mblock_size, config\u001b[39m.\u001b[39;49mblock_size))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/d_drive/Contlo/ContloFinal.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m                             \u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, config\u001b[39m.\u001b[39mblock_size, config\u001b[39m.\u001b[39mblock_size))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4194304 bytes."
     ]
    }
   ],
   "source": [
    "# Initializing the model from scratch/gpt2\n",
    "\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "block_size = 1024\n",
    "bias = False\n",
    "device = 'cuda'\n",
    "init_from = 'scratch' # can be scratch or from checkpoint\n",
    "out_dir = r'./checkpoints' # directory to save the trained model, for resuming training later.\n",
    "dropout = 0 # 0 for pretraining and >0.1 for finetuning and other tasks\n",
    "meta_vocab_size = None # if None, defaults to the full vocab size\n",
    "\n",
    "\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=50257, dropout=dropout) # start with model_args from command line\n",
    "initializer = GPT2ModelInitializer(init_from=init_from, model_args=model_args, out_dir=out_dir, device = device, dropout = dropout)\n",
    "model, model_args,_ = initializer.initialize_model()\n",
    "\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "\n",
    "# move the model to GPU\n",
    "model = model.to(device)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the sample prediction (using scratch and pretrained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making prediction from scratch model\n",
    "\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "block_size = 1024\n",
    "bias = False\n",
    "device = 'cuda'\n",
    "init_from = 'scratch' # can be scratch or from checkpoint\n",
    "out_dir = r'./checkpoints' # directory to save the trained model, for resuming training later.\n",
    "dropout = 0 # 0 for pretraining and >0.1 for finetuning and other tasks\n",
    "meta_vocab_size = None # if None, defaults to the full vocab size\n",
    "\n",
    "\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=50257, dropout=dropout) # start with model_args from command line\n",
    "initializer = GPT2ModelInitializer(init_from=init_from, model_args=model_args,meta_vocab_size=meta_vocab_size, out_dir=out_dir, device = device, dropout = dropout)\n",
    "model, model_args,_ = initializer.initialize_model()\n",
    "\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "\n",
    "# move the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()        # put in eval mode so that things like dropout are properly configured\n",
    "num_samples = 1     # number of samples to generate from the model\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8       # temperature is the randomness in the distribution. 0.8 is a good value to get diverse responses.\n",
    "top_k = 200             # top_k constrains the generated guesses to the top k guesses\n",
    "start = \"Hi my name is Udit Agarwal, final year student at IIT Jodhpur\" # can be any string\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "decode = lambda l: enc.decode(l)\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "\n",
    "# run generation , roughly takes 45s on NVIDIA GeForce GTX 1660 Ti \n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            print(decode(y[0].tolist()))\n",
    "            print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final output will look something like this \n",
    "\n",
    "![Imgur](https://i.imgur.com/kwTPgRm.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating text using pretrained model\n",
    "\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "block_size = 1024\n",
    "bias = False\n",
    "device = 'cuda'\n",
    "init_from = 'gpt2' # can be scratch or from checkpoint\n",
    "out_dir = r'./checkpoints' # directory to save the trained model, for resuming training later.\n",
    "dropout = 0 # 0 for pretraining and >0.1 for finetuning and other tasks\n",
    "meta_vocab_size = None # if None, defaults to the full vocab size\n",
    "\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=50257, dropout=dropout) # start with model_args from command line\n",
    "initializer = GPT2ModelInitializer(init_from=init_from, model_args=model_args,meta_vocab_size=meta_vocab_size, out_dir=out_dir, device = device, dropout = dropout)\n",
    "model, model_args,_ = initializer.initialize_model()\n",
    "\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "\n",
    "# move the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()        # put in eval mode so that things like dropout are properly configured\n",
    "num_samples = 1     # number of samples to generate from the model\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8       # temperature is the randomness in the distribution. 0.8 is a good value to get diverse responses.\n",
    "top_k = 200             # top_k constrains the generated guesses to the top k guesses\n",
    "start = \"Hi my name is Udit Agarwal\" # can be any string\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "decode = lambda l: enc.decode(l)\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "\n",
    "# run generation , roughly takes 45s on NVIDIA GeForce GTX 1660 Ti \n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            print(decode(y[0].tolist()))\n",
    "            print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final output here will look something like this:\n",
    "\n",
    "![Imgur](https://imgur.com/CUpowxA.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the task 1, I have implemented the GPT2 model (125 M parameters) from scratch.\n",
    "Resources:\n",
    "1. https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "2. https://github.com/karpathy/nanoGPT\n",
    "3. https://youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&feature=shared\n",
    "4. https://github.com/huggingface/transformers/blob/v4.35.2/src/transformers/models/gpt2/convert_gpt2_original_tf_checkpoint_to_pytorch.py\n",
    "\n",
    "The implementation is very much inspired from the one in nanoGPT. I have abstained from using any of the pre-built transformer library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Making the architectural changes in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotary Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the class implementing the rotary positional embeddings\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(RotaryEmbedding, self).__init__()\n",
    "        self.config = config\n",
    "        self.n_embd = config.n_embd\n",
    "        self.block_size = config.block_size\n",
    "        self.embedding = nn.Embedding(self.block_size, self.n_embd)\n",
    "        self.weight = self.embedding.weight\n",
    "\n",
    "    def forward(self, pos):\n",
    "        # Get the positional embeddings\n",
    "        pos_emb = self.embedding(pos)\n",
    "        # Apply rotary embeddings\n",
    "        pos_emb = self.apply_rotary(pos_emb)\n",
    "        return pos_emb\n",
    "\n",
    "    def apply_rotary(self, x):\n",
    "        # Apply rotary embeddings to the input tensor x.\n",
    "        freqs = self.get_freqs(x.size(1) // 2, x.device)\n",
    "\n",
    "        cosines = torch.cos(freqs.view(1, -1) * x)\n",
    "        sines = torch.sin(freqs.view(1, -1) * x)\n",
    "\n",
    "        cosines = cosines[:, 0::2]\n",
    "        sines = sines[:, 0::2]\n",
    "        return torch.cat([cosines, sines], dim=-1)\n",
    "\n",
    "    def get_freqs(self, size, device):\n",
    "        # Get the frequencies for rotary embeddings.\n",
    "        # return torch.exp(torch.arange(0, self.n_embd, 2, dtype=torch.float32, device=device).float() * -(torch.log(10000.0) / self.n_embd))\n",
    "        print (size)\n",
    "        freqs = torch.exp(torch.arange(0, self.n_embd, 1, dtype=torch.float32, device=device).float() * -(torch.log(torch.tensor(10000.0, dtype=torch.float32, device=device)) / self.n_embd))\n",
    "        print(\"Shape of freqs:\", freqs.shape)\n",
    "        return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Redifning the GPTconfig and GPT class to incorporate the roatary embeddings\n",
    "@dataclass    \n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "    rpe: bool = True # True: Positional embedding is rotational positional embedding (This bool variable is added to check if the user \n",
    "                                                                                        #wants rotary positonal embeddings or not)\n",
    "\n",
    "# Defining the GPT model\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        if config.rpe == False:\n",
    "            self.transformer = nn.ModuleDict(dict(\n",
    "                wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "                drop = nn.Dropout(config.dropout),\n",
    "                h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "                ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "            ))\n",
    "\n",
    "        else:\n",
    "            self.transformer = nn.ModuleDict(dict(\n",
    "                wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe = RotaryEmbedding(config),\n",
    "                drop = nn.Dropout(config.dropout),\n",
    "                h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "                ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "            ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def from_pretrained(model_type, override_args=None):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        override_args = override_args or {} # default to empty dict\n",
    "        # only dropout can be overridden see more notes below\n",
    "        assert all(k == 'dropout' for k in override_args)\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        config_args['bias'] = True # always True for GPT model checkpoints\n",
    "        # we can override the dropout rate, if desired\n",
    "        if 'dropout' in override_args:\n",
    "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args['dropout'] = override_args['dropout']\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 123.59M\n"
     ]
    }
   ],
   "source": [
    "## Calling the initializer function again\n",
    "\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "block_size = 1024\n",
    "bias = False\n",
    "device = 'cuda'\n",
    "init_from = 'scratch' # can be scratch or from checkpoint\n",
    "out_dir = r'./checkpoints' # directory to save the trained model, for resuming training later.\n",
    "dropout = 0 # 0 for pretraining and >0.1 for finetuning and other tasks\n",
    "rpe = True\n",
    "meta_vocab_size = None # if None, defaults to the full vocab size\n",
    "\n",
    "\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=50257, dropout=dropout, rpe = rpe) # start with model_args from command line\n",
    "initializer = GPT2ModelInitializer(init_from=init_from, model_args=model_args, out_dir=out_dir, device = device, dropout = dropout)\n",
    "model, model_args,_ = initializer.initialize_model()\n",
    "\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "\n",
    "# move the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code implements the GPT 2 with Rotary Positional Embeddings.\n",
    "References are:\n",
    "1. https://arxiv.org/pdf/2104.09864.pdf\n",
    "2. https://github.com/huggingface/transformers/blob/v4.35.2/src/transformers/models/roformer/modeling_roformer.py\n",
    "3. https://github.com/mistralai/mistral-src\n",
    "\n",
    "The implemetation is very much similar to Transformer's RoFormer. Implementing rotary positional embeddings might increase the complexity of training, as it involves additional mathematical operations to compute the rotational embeddings. Rotary positional embeddings are specifically designed to address the limitations of traditional positional embeddings in capturing long-term dependencies in sequences. The improvement in handling long-range dependencies can lead to better performance in tasks that require understanding context over extended sequences. Rotary positional embeddings are beneficial in varying sequence lenght problems, and understanding text over extended sequence.\n",
    "\n",
    "##### Challenges Faced: \n",
    "Since this was a recent paper, very few github repos were avaliable with the algorithm's implementation. It was hard code the exact maths and then match the size of tensors expected in matriz multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group Query Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to define the builder function and the Attention model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import einsum, rearrange\n",
    "# Group query attention\n",
    "def scaled_dot_product_gqa(\n",
    "    query: Tensor,\n",
    "    key: Tensor,\n",
    "    value: Tensor,\n",
    "    dropout: float = 0.0,\n",
    "    scale: Optional[float] = None,\n",
    "    mask: Optional[Tensor] = None,\n",
    "    is_causal: Optional[bool] = None,\n",
    "    need_weights: bool = False,\n",
    "    average_attn_weights: bool = False,\n",
    "    force_grouped: bool = False,\n",
    "):\n",
    "    \"\"\"Scaled dot product attention with support for grouped queries.\n",
    "\n",
    "    Einstein notation:\n",
    "    - b: batch size\n",
    "    - n / s: sequence length\n",
    "    - h: number of heads\n",
    "    - g: number of groups\n",
    "    - d: dimension of query/key/value\n",
    "\n",
    "    Args:\n",
    "        query: Query tensor of shape (b, n, h, d)\n",
    "        key: Key tensor of shape (b, s, h, d)\n",
    "        value: Value tensor of shape (b, s, h, d)\n",
    "        dropout: Dropout probability (default: 0.0)\n",
    "        scale: Scale factor for query (default: d_query ** 0.5)\n",
    "        mask: Mask tensor of shape (b, n, s) or (b, s). If 'ndim == 2', the mask is\n",
    "            applied to all 'n' rows of the attention matrix. (default: None)\n",
    "        force_grouped: If True, apply grouped-query attention even if the number of\n",
    "            heads is equal for query, key, and value. (default: False)\n",
    "\n",
    "    Returns:\n",
    "        2-tuple of:\n",
    "        - Attention output with shape (b, n, h, d)\n",
    "        - (Optional) Attention weights with shape (b, h, n, s). Only returned if\n",
    "          'need_weights' is True.\n",
    "    \"\"\"\n",
    "    if (mask is not None) and (is_causal is not None):\n",
    "        raise ValueError(\n",
    "            \"Only one of 'mask' and 'is_causal' should be provided, but got both.\"\n",
    "        )\n",
    "    elif not query.ndim == key.ndim == value.ndim == 4:\n",
    "        raise ValueError(\n",
    "            f\"Expected query, key, and value to be 4-dimensional, but got shapes \"\n",
    "            f\"{query.shape}, {key.shape}, and {value.shape}.\"\n",
    "        )\n",
    "\n",
    "    # Move sequence length dimension to axis 2.\n",
    "    # This makes the attention operations below *much* faster.\n",
    "    query = rearrange(query, \"b n h d -> b h n d\")\n",
    "    key = rearrange(key, \"b s h d -> b h s d\")\n",
    "    value = rearrange(value, \"b s h d -> b h s d\")\n",
    "\n",
    "    bq, hq, nq, dq = query.shape\n",
    "    bk, hk, nk, dk = key.shape\n",
    "    bv, hv, nv, dv = value.shape\n",
    "    if not (bq == bk == bv and dq == dk == dv):\n",
    "        raise ValueError(\n",
    "            \"Expected query, key, and value to have the same batch size (dim=0) and \"\n",
    "            f\"embedding dimension (dim=3), but got query: {query.shape}, \"\n",
    "            f\"key: {key.shape}, and value: {value.shape}.\"\n",
    "        )\n",
    "    elif (hk != hv) or (nk != nv):\n",
    "        raise ValueError(\n",
    "            \"Expected key and value to have the same size in dimensions 1 and 2, but \"\n",
    "            f\"got key: {key.shape} and value: {value.shape}.\"\n",
    "        )\n",
    "    elif hq % hk != 0:\n",
    "        raise ValueError(\n",
    "            \"Expected query heads to be a multiple of key/value heads, but got \"\n",
    "            f\"query: {query.shape} and key/value: {key.shape}.\"\n",
    "        )\n",
    "\n",
    "    if scale is None:\n",
    "        scale = query.size(-1) ** 0.5\n",
    "    query = query / scale\n",
    "\n",
    "    num_head_groups = hq // hk\n",
    "    if num_head_groups > 1 or force_grouped:\n",
    "        # Separate the query heads into 'num_head_groups' chunks, and fold the group\n",
    "        # dimension into the batch dimension.  This allows us to compute the attention\n",
    "        # for each head in parallel, then sum over all of the groups at the end.\n",
    "        query = rearrange(query, \"b (h g) n d -> b g h n d\", g=num_head_groups)\n",
    "        similarity = einsum(query, key, \"b g h n d, b h s d -> b h n s\")\n",
    "    else:\n",
    "        # If the number of query/key heads is equal, we can skip grouping the queries,\n",
    "        # and just use the standard sdot product attention.\n",
    "        similarity = einsum(query, key, \"b h n d, b h s d -> b h n s\")\n",
    "\n",
    "    if is_causal:\n",
    "        # Mask out the upper triangular portion of the attention matrix. This prevents\n",
    "        # the model from attending to tokens in the future.\n",
    "        mask = torch.ones(\n",
    "            (bq, nq, nk),\n",
    "            device=query.device,\n",
    "            dtype=torch.bool,\n",
    "        ).tril_()\n",
    "\n",
    "    if mask is not None:\n",
    "        # Expand mask to match the shape of the attention matrix.\n",
    "        # If mask is 2D, assume that it is applied to the key/value sequence dimension.\n",
    "        # Else if mask is 3D, assume that it is applied to the query/key/value sequence\n",
    "        # dimension for all attention heads.\n",
    "        #\n",
    "        # Users could also provide a 4D mask, which is applied to the query/key/value\n",
    "        # sequence dimension for each attention head (though I don't have a particular\n",
    "        # use case in mind for that).\n",
    "        if mask.ndim == 2:\n",
    "            mask = rearrange(mask, \"b s -> b () () s\")\n",
    "        elif mask.ndim == 3:\n",
    "            mask = rearrange(mask, \"b n s -> b () n s\")\n",
    "        # Mask similarity values by setting them to negative infinity.  This guarantees\n",
    "        # that they will not contribute to the softmax computation below.\n",
    "        similarity.masked_fill_(~mask, torch.finfo(similarity.dtype).min)\n",
    "\n",
    "    attention = F.softmax(similarity / scale, dim=-1)\n",
    "    if dropout > 0.0:\n",
    "        attention = F.dropout(attention, p=dropout)\n",
    "\n",
    "    # Apply attention matrix to the value Tensor.\n",
    "    out = einsum(attention, value, \"b h n s, b h s d -> b h n d\")\n",
    "    # Move head dimension back to axis 2\n",
    "    out = rearrange(out, \"b h n d -> b n h d\")\n",
    "\n",
    "    attn_weights: Optional[Tensor] = None\n",
    "    if need_weights:\n",
    "        # Move the sequence dimensions back to positions 1, 2.  Move the head dimension\n",
    "        # to position 3.  This more closely matches the return shape of the attention\n",
    "        # output: (b, n, h, d).\n",
    "        attn_weights = rearrange(attention, \"b h n s -> b n s h\")\n",
    "        if average_attn_weights:\n",
    "            attn_weights = attn_weights.mean(dim=1)\n",
    "\n",
    "    return out, attn_weights\n",
    "\n",
    "\n",
    "class MultiheadGQA(nn.Module):\n",
    "    \"\"\"Multi-head grouped query attention (GQA) layer.\n",
    "\n",
    "    Reference:\n",
    "        \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\"\n",
    "        https://arxiv.org/pdf/2305.13245v1.pdf\n",
    "\n",
    "    GQA is a variant of multihead attention (MHA) that uses fewer write heads\n",
    "    (key / value) than query heads.  GQA can be viewed as a generalization of\n",
    "    multi-query attention (MQA), which uses a single write head. GQA and MQA give\n",
    "    significant speedups over standard MHA in decoder layers, with minimal loss in\n",
    "    accuracy. In the paper, GQA is shown to be more accurate than MQA, while still\n",
    "    having a significant speedup over MHA.\n",
    "\n",
    "    NOTE: The original authors only benchmark GQA by adapting the T5 (XL or XXL) model\n",
    "    from MHA to GQA.  As a result, they do not mention parameter initialization or\n",
    "    layer normalization strategies.  I follow the best practices laid out in the\n",
    "    MAGNETO paper, which improves Transformer performance through better parameter\n",
    "    initialization and layer norm placement.  See:\n",
    "        https://arxiv.org/pdf/2210.06423.pdf, Fig. 2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = True,\n",
    "        layer_norm: bool = True,\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "        gamma_init: float = 1.0,\n",
    "        device: Optional[Union[torch.device, str]] = None,\n",
    "        dtype: Optional[torch.dtype] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.query_heads = config.n_head\n",
    "        self.kv_heads = config.n_head\n",
    "        self.dropout = dropout\n",
    "        self.layer_norm = layer_norm\n",
    "        self.gamma_init = gamma_init\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        query_heads = config.n_head\n",
    "        kv_heads = config.n_head\n",
    "        embed_dim = config.n_embd\n",
    "        if self.query_heads % self.kv_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"query_heads ({query_heads}) must be divisible by \"\n",
    "                f\"kv_heads ({kv_heads})\"\n",
    "            )\n",
    "        elif (embed_dim % self.query_heads != 0) or (embed_dim % self.kv_heads != 0):\n",
    "            raise ValueError(\n",
    "                f\"embed_dim ({embed_dim}) must be divisible by \"\n",
    "                f\"query_heads ({query_heads}) and kv_heads ({kv_heads})\"\n",
    "            )\n",
    "\n",
    "        head_dim = embed_dim // query_heads\n",
    "        if not head_dim % 8 == 0:\n",
    "            raise ValueError(\n",
    "                f\"head_dim (embed_dim / num_heads = {head_dim}) must be divisible by 8\"\n",
    "            )\n",
    "        if not head_dim <= 128:\n",
    "            raise ValueError(\n",
    "                f\"head_dim (embed_dim / num_heads = {head_dim}) must be <= 128\"\n",
    "            )\n",
    "\n",
    "        # Query projection layer is the same as in vanilla MHA.\n",
    "        self.q_proj = nn.Linear(\n",
    "            embed_dim, embed_dim, bias=bias, device=device, dtype=dtype\n",
    "        )\n",
    "        # Key/value projection layers have a smaller output dimension, so that\n",
    "        # the we have fewer key/value attention heads after reshaping.\n",
    "        kv_embed_dim = embed_dim // query_heads * kv_heads\n",
    "        self.k_proj = nn.Linear(\n",
    "            embed_dim, kv_embed_dim, bias=bias, device=device, dtype=dtype\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            embed_dim, kv_embed_dim, bias=bias, device=device, dtype=dtype\n",
    "        )\n",
    "        self.norm: Optional[nn.LayerNorm] = None\n",
    "        if layer_norm:\n",
    "            self.norm = nn.LayerNorm(\n",
    "                kv_embed_dim, eps=layer_norm_eps, device=device, dtype=dtype\n",
    "            )\n",
    "        # Grouped attention output will have the same embedding dimension as the\n",
    "        # key/value Tensors.  So the output projection layer needs to accept the\n",
    "        # same dimension (kv_embed_dim).\n",
    "        self.out_proj = nn.Linear(\n",
    "            kv_embed_dim, embed_dim, bias=bias, device=device, dtype=dtype\n",
    "        )\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_normal_(self.q_proj.weight)\n",
    "        if self.q_proj.bias is not None:\n",
    "            nn.init.constant_(self.q_proj.bias, 0)\n",
    "        nn.init.xavier_normal_(self.k_proj.weight)\n",
    "        if self.k_proj.bias is not None:\n",
    "            nn.init.constant_(self.k_proj.bias, 0)\n",
    "\n",
    "        # NOTE: We follow the initialization strategy from MAGNETO.  See:\n",
    "        # https://arxiv.org/pdf/2210.06423.pdf, Fig. 2\n",
    "        # Gain (self.gamma_init) should be provided as a keyword argument when\n",
    "        # initializing the larger Transformer model, since it requires knowledge\n",
    "        # of the number of encoder/decoder layers in the model.\n",
    "\n",
    "        nn.init.xavier_normal_(self.v_proj.weight, gain=self.gamma_init)\n",
    "        if self.v_proj.bias is not None:\n",
    "            nn.init.constant_(self.v_proj.bias, 0)\n",
    "        nn.init.xavier_normal_(self.out_proj.weight, gain=self.gamma_init)\n",
    "        if self.out_proj.bias is not None:\n",
    "            nn.init.constant_(self.out_proj.bias, 0)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        need_weights: bool = False,\n",
    "        # TODO\n",
    "        # attn_mask: Optional[Tensor] = None,\n",
    "        is_causal: bool = False,\n",
    "        average_attn_weights: bool = False,\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        # Notation:\n",
    "        #   b - batch size\n",
    "        #   n - sequence length\n",
    "        #   h - number of heads\n",
    "        #   d - embedding dimension\n",
    "        # Input shape: (b, n, d)\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        query, key, value  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        key = k.view(B, T, C)\n",
    "        query = q.view(B, T, C) \n",
    "        value = v.view(B, T, C)\n",
    "        q: Tensor = self.q_proj(query)\n",
    "        k: Tensor = self.k_proj(key)\n",
    "        v: Tensor = self.v_proj(value)\n",
    "\n",
    "        # Unfold 'd' dimension into 'h' separate attention heads.\n",
    "        q = rearrange(q, \"b n (h d) -> b n h d\", h=self.query_heads)\n",
    "        k = rearrange(k, \"b n (h d) -> b n h d\", h=self.kv_heads)\n",
    "        v = rearrange(v, \"b n (h d) -> b n h d\", h=self.kv_heads)\n",
    "        # Apply attention, then fold 'h' attention heads back into 'd'.\n",
    "        x, attn = scaled_dot_product_gqa(\n",
    "            query=q,\n",
    "            key=k,\n",
    "            value=v,\n",
    "            # TODO\n",
    "            # mask=attn_mask,\n",
    "            is_causal=is_causal,\n",
    "            need_weights=need_weights,\n",
    "            average_attn_weights=average_attn_weights,\n",
    "            force_grouped=False,\n",
    "        )\n",
    "        x = rearrange(x, \"b n h d -> b n (h d)\")\n",
    "        if self.layer_norm:\n",
    "            assert self.norm is not None\n",
    "            x = self.norm(x)\n",
    "        # Linear projection on attention outputs.\n",
    "        x = self.out_proj(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Upating the GPTconfig class, the bool values swe when True, indicates the code to implement sliding window attention'''\n",
    "\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0 # To be kept 0 during pre-training and to be made >0.1 for fine tuning tasks\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "    rpe: bool = True # True: Positional embedding is rotational positional embedding\n",
    "    swe: bool = True # True: Sliding window attention is the attention mechnaism\n",
    "    gqa: bool = True # True: Group query attention is the attention mechanism\n",
    "\n",
    "'''To incorporate the group query attention the block class is updated as follows. An if condition is intrduced that if swe is true to call \n",
    "MultiheadGQAClass'''\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        if config.gqa == True:\n",
    "            self.attn = MultiheadGQA(config)\n",
    "        # elif config.swe == True:\n",
    "        #     self.attn = SlidingCausalSelfAttention(config, 2)\n",
    "        else:\n",
    "            self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "'''The rest of the GPT class remains the same with no changes.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''initializing the model using with Group Query Attention mechanism. '''\n",
    "\n",
    "## Calling the initializer function again\n",
    "\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "block_size = 1024\n",
    "bias = False\n",
    "device = 'cuda'\n",
    "init_from = 'scratch' # can be scratch or from checkpoint\n",
    "out_dir = r'./checkpoints' # directory to save the trained model, for resuming training later.\n",
    "dropout = 0 # 0 for pretraining and >0.1 for finetuning and other tasks\n",
    "rpe = False\n",
    "swe = False # Indicates that we want sliding window attention\n",
    "gqa = True\n",
    "meta_vocab_size = None # if None, defaults to the full vocab size\n",
    "\n",
    "\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=50257, dropout=dropout, rpe = rpe, swe = swe, gqa = gqa) # start with model_args from command line\n",
    "initializer = GPT2ModelInitializer(init_from=init_from, model_args=model_args, out_dir=out_dir, device = device, dropout = dropout)\n",
    "model, model_args,_ = initializer.initialize_model()\n",
    "\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "\n",
    "# move the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the implementation of Group Query Attention mechanism.\n",
    "Resources:\n",
    "1. https://github.com/fkodom/grouped-query-attention-pytorch\n",
    "2. https://arxiv.org/pdf/2305.13245v2.pdf\n",
    "\n",
    "##### Pitfalls\n",
    "This could make the model more computationally expensive to train and run, potentially limiting its practicality for real-world applications. The additional parameters introduced by group query attention could increase the model's susceptibility to overfitting, especially when dealing with limited or noisy training data. \n",
    "\n",
    "##### Improvements\n",
    "Group query attention could enhance the model's ability to capture long-range dependencies within the input sequence. This is particularly beneficial for tasks like language modeling and machine translation, where understanding distant relationships between words or phrases is crucial. By dividing the input sequence into groups, group query attention could improve the model's efficiency in processing long sequences. This could make it more applicable to tasks like summarization and question answering, where dealing with extensive text is common.\n",
    "\n",
    "##### Difficulties\n",
    "\n",
    "As this was recent paper, only one github repo could be found with its implementation, this was one of the challenge.  This was fairly simple as compared to Sliding Window Attetion model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sliding Window Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing sliding window involves implementing several other funcitons, I will be defining them before doing the architectural changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _skew(x, direction, padding_value):\n",
    "    '''Convert diagonals into columns (or columns into diagonals depending on `direction`)'''\n",
    "    x_padded = F.pad(x, direction, value=padding_value)\n",
    "    x_padded = x_padded.view(*x_padded.size()[:-2], x_padded.size(-1), x_padded.size(-2))\n",
    "    return x_padded\n",
    "\n",
    "def _skew2(x, padding_value):\n",
    "    '''shift every row 1 step to right converting columns into diagonals'''\n",
    "    # X = B x C x M x L\n",
    "    B, C, M, L = x.size()\n",
    "    x = F.pad(x, (0, M + 1), value=padding_value)  # B x C x M x (L+M+1)\n",
    "    x = x.view(B, C, -1)  # B x C x ML+MM+M\n",
    "    x = x[:, :, :-M]  # B x C x ML+MM\n",
    "    x = x.view(B, C, M, M + L)  # B x C, M x L+M\n",
    "    x = x[:, :, :, :-1]\n",
    "    return x\n",
    "\n",
    "def _get_invalid_locations_mask_fixed_dilation(seq_len: int, w: int, d: int):\n",
    "    diagonals_list = []\n",
    "    for j in range(-d * w, d, d):\n",
    "        diagonal_mask = torch.zeros(seq_len, device='cpu', dtype=torch.uint8)\n",
    "        diagonal_mask[:-j] = 1\n",
    "        diagonals_list.append(diagonal_mask)\n",
    "    return torch.stack(diagonals_list, dim=-1)\n",
    "\n",
    "# @lru_cache()\n",
    "def _get_invalid_locations_mask(w: int, d: int, autoregressive: bool, device: str):\n",
    "    if isinstance(d, int):\n",
    "        affected_seq_len = w * d\n",
    "        mask = _get_invalid_locations_mask_fixed_dilation(affected_seq_len, w, d)\n",
    "        mask = mask[None, :, None, :]\n",
    "    else:\n",
    "        d_arr = np.array(d)\n",
    "        affected_seq_len = w * d_arr.max()\n",
    "        head_masks = []\n",
    "        d_list = d.cpu().numpy().tolist()\n",
    "        for d in d_list:\n",
    "            one_head_mask = _get_invalid_locations_mask_fixed_dilation(affected_seq_len, w, d)\n",
    "            head_masks.append(one_head_mask)\n",
    "        mask = torch.stack(head_masks, dim=-2)\n",
    "        mask = mask[None, :, :, :]\n",
    "\n",
    "    ending_mask = None if autoregressive else mask.flip(dims=(1, 3)).bool().to(device)\n",
    "    return affected_seq_len, mask.bool().to(device), ending_mask\n",
    "\n",
    "def mask_invalid_locations(input_tensor: torch.Tensor, w: int, d: int, autoregressive: bool) -> torch.Tensor:\n",
    "    affected_seq_len, beginning_mask, ending_mask = _get_invalid_locations_mask(w, d, autoregressive, input_tensor.device)\n",
    "    seq_len = input_tensor.size(1)\n",
    "    beginning_input = input_tensor[:, :affected_seq_len, :, :w+1]\n",
    "    beginning_mask = beginning_mask[:, :seq_len].expand(beginning_input.size())\n",
    "    beginning_input.masked_fill_(beginning_mask, -float('inf'))\n",
    "    if not autoregressive:\n",
    "        ending_input = input_tensor[:, -affected_seq_len:, :, -(w+1):]\n",
    "        ending_mask = ending_mask[:, -seq_len:].expand(ending_input.size())\n",
    "        ending_input.masked_fill_(ending_mask, -float('inf'))\n",
    "\n",
    "\n",
    "def _chunk(x, w):\n",
    "    '''convert into overlapping chunkings. Chunk size = 2w, overlap size = w'''\n",
    "    # non-overlapping chunks of size = 2w\n",
    "    x = x.view(x.size(0), x.size(1) // (w * 2), w * 2, x.size(2))\n",
    "    # use `as_strided` to make the chunks overlap with an overlap size = w\n",
    "    chunk_size = list(x.size())\n",
    "    chunk_size[1] = chunk_size[1] * 2 - 1\n",
    "    chunk_stride = list(x.stride())\n",
    "    chunk_stride[1] = chunk_stride[1] // 2\n",
    "    return x.as_strided(size=chunk_size, stride=chunk_stride)\n",
    "\n",
    "def sliding_chunks_matmul_pv(prob: torch.Tensor, v: torch.Tensor, w: int):\n",
    "    '''Same as sliding_chunks_matmul_qk but for prob and value tensors. It is expecting the same output\n",
    "    format from sliding_chunks_matmul_qk'''\n",
    "    bsz, seqlen, num_heads, head_dim = v.size()\n",
    "    assert seqlen % (w * 2) == 0\n",
    "    assert prob.size()[:3] == v.size()[:3]\n",
    "    assert prob.size(3) == 2 * w + 1\n",
    "    chunks_count = seqlen // w - 1\n",
    "    # group bsz and num_heads dimensions into one, then chunk seqlen into chunks of size 2w\n",
    "    chunk_prob = prob.transpose(1, 2).reshape(bsz * num_heads, seqlen // w, w, 2 * w + 1)\n",
    "    # group bsz and num_heads dimensions into one\n",
    "    v = v.transpose(1, 2).reshape(bsz * num_heads, seqlen, head_dim)\n",
    "    # pad seqlen with w at the beginning of the sequence and another w at the end\n",
    "    padded_v = F.pad(v, (0, 0, w, w), value=-1)\n",
    "    # chunk padded_v into chunks of size 3w and an overlap of size w\n",
    "    chunk_v_size = (bsz * num_heads, chunks_count + 1, 3 * w, head_dim)\n",
    "    chunk_v_stride = padded_v.stride()\n",
    "    chunk_v_stride = chunk_v_stride[0], w * chunk_v_stride[1], chunk_v_stride[1], chunk_v_stride[2]\n",
    "    chunk_v = padded_v.as_strided(size=chunk_v_size, stride=chunk_v_stride)\n",
    "    skewed_prob = _skew2(chunk_prob, padding_value=0)\n",
    "    context = torch.einsum('bcwd,bcdh->bcwh', (skewed_prob, chunk_v))\n",
    "    return context.view(bsz, num_heads, seqlen, head_dim).transpose(1, 2)\n",
    "\n",
    "def sliding_chunks_matmul_qk(q: torch.Tensor, k: torch.Tensor, w: int, padding_value: float):\n",
    "    '''Matrix multiplication of query x key tensors using a sliding window attention pattern.\n",
    "    This implementation splits the input into overlapping chunks of size 2w (e.g., 512 for pretrained Longformer)\n",
    "    with an overlap size w'''\n",
    "    bsz, num_heads, seqlen, head_dim = q.size()\n",
    "    # print (q.size())\n",
    "    # print (k.size())\n",
    "    assert seqlen % (w * 2) == 0\n",
    "    assert q.size() == k.size()\n",
    "    chunks_count = seqlen // w - 1\n",
    "    # group bsz and num_heads dimensions into one, then chunk seqlen into chunks of size w * 2\n",
    "    q = q.transpose(1, 2).reshape(bsz * num_heads, seqlen, head_dim)\n",
    "    k = k.transpose(1, 2).reshape(bsz * num_heads, seqlen, head_dim)\n",
    "    chunk_q = _chunk(q, w)\n",
    "    chunk_k = _chunk(k, w)\n",
    "    # matrix multiplication\n",
    "    # bcxd: bsz*num_heads x chunks x 2w x head_dim\n",
    "    # bcyd: bsz*num_heads x chunks x 2w x head_dim\n",
    "    # bcxy: bsz*num_heads x chunks x 2w x 2w\n",
    "    chunk_attn = torch.einsum('bcxd,bcyd->bcxy', (chunk_q, chunk_k))\n",
    "\n",
    "    # convert diagonals into columns\n",
    "    diagonal_chunk_attn = _skew(chunk_attn, direction=(0, 0, 0, 1), padding_value=padding_value)\n",
    "    # allocate space for the overall attention matrix where the chunks are combined\n",
    "    diagonal_attn = diagonal_chunk_attn.new_empty((bsz * num_heads, chunks_count + 1, w, w * 2 + 1))\n",
    "    # copy parts from diagonal_chunk_attn into the combined matrix of attentions\n",
    "    # - copying the main diagonal and the upper triangle\n",
    "    diagonal_attn[:, :-1, :, w:] = diagonal_chunk_attn[:, :, :w, :w + 1]\n",
    "    diagonal_attn[:, -1, :, w:] = diagonal_chunk_attn[:, -1, w:, :w + 1]\n",
    "    # - copying the lower triangle\n",
    "    diagonal_attn[:, 1:, :, :w] = diagonal_chunk_attn[:, :, - (w + 1):-1, w + 1:]\n",
    "    diagonal_attn[:, 0, 1:w, 1:w] = diagonal_chunk_attn[:, 0, :w - 1, 1 - w:]\n",
    "\n",
    "    # separate bsz and num_heads dimensions again\n",
    "    diagonal_attn = diagonal_attn.view(bsz, num_heads, seqlen, 2 * w + 1).transpose(2, 1)\n",
    "\n",
    "    mask_invalid_locations(diagonal_attn, w, 1, False)\n",
    "    return diagonal_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have defined our necessary funcitons we will make changes in the GPTconfig,  GPT class to incorporate the architectural change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Class for calculating the attention values, given the q and k tensors. \n",
    "\n",
    "class SlidingAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, sliding_window_size, dropout):\n",
    "        super(SlidingAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.sliding_window_size = sliding_window_size\n",
    "        self.dropout = dropout\n",
    "        # Define your sliding attention parameters and layers here\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Assuming input has shape (batch_size, sequence_length, hidden_size)\n",
    "        bsz, num_head, seqlen, hidden_size = input.size()\n",
    "        # Reshape the input to (batch_size * num_heads, sequence_length, head_dim)\n",
    "        q = input\n",
    "        k = input\n",
    "        attn_weight = sliding_chunks_matmul_qk(q, k, self.sliding_window_size, padding_value=0)\n",
    "        # Reshape the output back to (batch_size, num_heads, sequence_length, head_dim)\n",
    "        # print (attn_output.shape)\n",
    "        attention_dilation = 1\n",
    "        mask_invalid_locations(attn_weight, self.sliding_window_size, attention_dilation, False)\n",
    "        # attn_weight = attn_weight.reshape(bsz, self.num_heads, seqlen, -1).contiguous()\n",
    "        attn_weights_float = F.softmax(attn_weight, dim=-1, dtype=torch.float32)\n",
    "        attn_weight = attn_weights_float.type_as(attn_weight)\n",
    "        attn_probs = F.dropout(attn_weights_float.type_as(attn_weight), p=self.dropout, training=self.training)\n",
    "        return attn_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Class similar to CausalSelfAttention class that you have seen earlier, but it is calculating the attention values using the sliding attention class\n",
    "and then multiplying the attention values with the v tensor to generate the final output'''\n",
    "\n",
    "class SlidingCausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config, sliding_window_size = 512):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.dropout = config.dropout\n",
    "        self.sliding_attention = SlidingAttention(config.n_embd, config.n_head, sliding_window_size, self.dropout)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        \n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        self.window = sliding_window_size\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        # Sliding Attention\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        y = self.sliding_attention(q)\n",
    "\n",
    "        v = v.reshape(T, B, self.n_head, C // self.n_head).transpose(0, 1)\n",
    "        attn = 0\n",
    "        attn += sliding_chunks_matmul_pv(y, v, self.window)\n",
    "        attn = attn.type_as(x.transpose(0, 1))\n",
    "        assert list(attn.size()) == [B, T, self.n_head, C // self.n_head]\n",
    "\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Upating the GPTconfig class, the bool values swe when True, indicates the code to implement sliding window attention'''\n",
    "\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0 # To be kept 0 during pre-training and to be made >0.1 for fine tuning tasks\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "    rpe: bool = True # True: Positional embedding is rotational positional embedding\n",
    "    swe: bool = True # True: Sliding window attention is the attention mechnaism\n",
    "\n",
    "'''To incorporate the Sliding window attention the block class is updated as follows. An if condition is intrduced that if swe is true to call \n",
    "SlidingCausalAttentionWindowClass'''\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        if config.swe == False:\n",
    "            self.attn = CausalSelfAttention(config)\n",
    "        else:\n",
    "            self.attn = SlidingCausalSelfAttention(config, 2)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "'''The rest of the GPT class remains the same with no changes.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''initializing the model using with Sliding Window Attention mechanism. '''\n",
    "\n",
    "## Calling the initializer function again\n",
    "\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "block_size = 1024\n",
    "bias = False\n",
    "device = 'cuda'\n",
    "init_from = 'scratch' # can be scratch or from checkpoint\n",
    "out_dir = r'./checkpoints' # directory to save the trained model, for resuming training later.\n",
    "dropout = 0 # 0 for pretraining and >0.1 for finetuning and other tasks\n",
    "rpe = False\n",
    "swe = True # Indicates that we want sliding window attention\n",
    "meta_vocab_size = None # if None, defaults to the full vocab size\n",
    "\n",
    "\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=50257, dropout=dropout, rpe = rpe, swe = swe) # start with model_args from command line\n",
    "initializer = GPT2ModelInitializer(init_from=init_from, model_args=model_args, out_dir=out_dir, device = device, dropout = dropout)\n",
    "model, model_args,_ = initializer.initialize_model()\n",
    "\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "\n",
    "# move the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will implement our Sliding Window Attention in our GPT 2 model. \n",
    "Resources:\n",
    "1. https://arxiv.org/pdf/2004.05150v2.pdf\n",
    "2. https://github.com/allenai/longformer/blob/master/longformer/longformer.py\n",
    "\n",
    "The implementation is very much inspired from longformer.\n",
    "\n",
    "##### Pitfalls:\n",
    "Sliding window attention restricts the model's ability to consider global context information. If the fixed-size window is too small, the model may struggle to capture long-range dependencies and understand the context effectively. Tasks that inherently require a broader context for accurate predictions or understanding might suffer from the limitations imposed by the sliding window.Choosing an appropriate window size and other hyperparameters for sliding window attention becomes really tough, I faced the same challenge while implementing this code. The window size play a key role in the attention tensor shape as well as the information retained.\n",
    "\n",
    "##### Potential Improvements:\n",
    "One of the primary motivations for sliding window attention is to reduce the computational complexity of the attention mechanism. This can lead to faster training and inference times, making the model more efficient, especially in resource-constrained environments. Sliding window attention may enable better parallelization during training, as the attention computation for different parts of the sequence can be performed independently. This can lead to faster training times on hardware that supports parallel processing.\n",
    "\n",
    "##### Challenges Faced: \n",
    "Longformer was the only github that I could find with the implementation of the algorithm. The comments provided made the code easy to understand but the implementation got tough due the window size. The window size was affecting the size of attention matrix that we were getting hence making it tough for me to perform the matrix multiplication of attention with values.  Similar such error persisted throughout the implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Training Loop Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single GPU training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare the Shakespeare dataset for character-level language modeling.\n",
    "So instead of encoding with GPT-2 BPE tokens, we just map characters to ints.\n",
    "Will save train.bin, val.bin containing the ids, and meta.pkl containing the\n",
    "encoder and decoder and some other related info.\n",
    "\"\"\"\n",
    "# if data folder doesn't exist, create it\n",
    "\n",
    "data_path = r'./data'\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "\n",
    "input_path = r'./data/shakespeare_char/' \n",
    "if not os.path.exists(input_path):\n",
    "    os.mkdir(input_path)\n",
    "\n",
    "# download the tiny shakespeare dataset\n",
    "input_file_path = os.path.join(os.path.dirname(input_path), 'input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")\n",
    "\n",
    "# get all the unique characters that occur in this text\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# create the train and test splits\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode both to integers\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(os.path.join(os.path.dirname(input_path), 'train.bin'))\n",
    "val_ids.tofile(os.path.join(os.path.dirname(input_path), 'val.bin'))\n",
    "\n",
    "# save the meta information as well, to help us encode/decode later\n",
    "meta = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'itos': itos,\n",
    "    'stoi': stoi,\n",
    "}\n",
    "with open(os.path.join(os.path.dirname(input_path), 'meta.pkl'), 'wb') as f:\n",
    "    pickle.dump(meta, f)\n",
    "\n",
    "# length of dataset in characters:  1115394\n",
    "# all the unique characters:\n",
    "#  !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
    "# vocab size: 65\n",
    "# train has 1003854 tokens\n",
    "# val has 111540 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Parameters and Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default config values designed to train a gpt2 (124M) on Shakespeare\n",
    "\n",
    "# I/O\n",
    "out_dir = r'./checkpoints'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
    "\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "block_size = 1024\n",
    "bias = False\n",
    "dropout = 0.0\n",
    "\n",
    "# data\n",
    "dataset = 'shakespeare_char' \n",
    "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 1024\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "# # -----------------------------------------------------------------------------\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# # -----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "various inits, derived attributes, I/O setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_process = True\n",
    "seed_offset = 0\n",
    "\n",
    "tokens_per_iter = gradient_accumulation_steps * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poor man's data loader\n",
    "data_dir = os.path.join('data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to derive vocab_size from the dataset and Initializing model\n",
    "\n",
    "##### Note: \n",
    "The GPT and GPT config class for the below cell should be the final updates classes after all the architecural changes\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout, rpe = False, swe = False) # start with model_args from command line\n",
    "\n",
    "initializer = GPT2ModelInitializer(init_from=init_from, model_args=model_args,meta_vocab_size=meta_vocab_size, out_dir=out_dir, device = device, dropout = dropout)\n",
    "model, model_args, checkpoint= initializer.initialize_model()\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# initializing num iter and best val loss for scratch training, will get overwritten if resuming\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(beta1, beta2), weight_decay=weight_decay)\n",
    "\n",
    "if init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "checkpoint = None # free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "\n",
    "running_mfu = -1.0 \n",
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code provided is the training implementation on single GPU. \n",
    "Resources:\n",
    "\n",
    "1. PyTorch's DDP tutorial: https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\n",
    "2. Gupta et al., 2020, Training GPT-3 Like Models on a Single Machine: https://arxiv.org/pdf/2101.06840.pdf\n",
    "3. nanoGPT: https://github.com/karpathy/nanoGPT\n",
    "\n",
    "The code is very much inspired from nanoGPT. Sadly this could not be completely run on my laptop becuase of graphic card/memory constraints. Here is a snap shot of few iteration that were successfully completed.\n",
    "\n",
    "![Imgur](https://imgur.com/i0NNc0H.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributed Data Parallel Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = True # use PyTorch 2.0 to compile the model to be faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "# If to check if DDP is possible\n",
    "if ddp:\n",
    "    init_process_group(backend=backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "    seed_offset = ddp_rank # each process gets a different seed\n",
    "    # world_size number of processes will be training simultaneously, so we can scale\n",
    "    # down the desired gradient accumulation iterations per process proportionally\n",
    "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
    "    gradient_accumulation_steps //= ddp_world_size\n",
    "\n",
    "\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# poor man's data loader\n",
    "data_dir = os.path.join('data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# attempt to derive vocab_size from the dataset\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout, rpe = False, swe = False) # start with model_args from command line\n",
    "\n",
    "initializer = GPT2ModelInitializer(init_from=init_from, model_args=model_args,meta_vocab_size=meta_vocab_size, out_dir=out_dir, device = device, dropout = dropout)\n",
    "model, model_args, checkpoint= initializer.initialize_model()\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "if init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "# compile the model\n",
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model) # requires PyTorch 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# training loop\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
    "running_mfu = -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        \n",
    "        # in DDP training we only need to sync gradients at the last micro step.\n",
    "        # the official way to do this is with model.no_sync() context manager, but\n",
    "        # I really dislike that this bloats the code and forces us to repeat code\n",
    "        # looking at the source of that context manager, it just toggles this variable\n",
    "        model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break\n",
    "\n",
    "destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully Sharded Data Parallel (FSDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I did not have multiple gpus access this was tough for me to implement. I believe with a bit more time and learning and access to gpus I will be able to implement it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a challenging task, learned a lot, revised my concepts on attention, query, key value and learned some new concepts like Group Query Attention. The assigment helped me in learning NLP a better way the resources shared were helpful in all ways. \n",
    "\n",
    "Thank You team for the awesome task !!!\n",
    "\n",
    "### Thank YOU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
